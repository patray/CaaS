<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="copyright" content="(C) Copyright 2019, Teradata. All Rights Reserved." />
<meta name="DC.rights.owner" content="(C) Copyright 2019, Teradata. All Rights Reserved." />
<meta name="DC.Type" content="concept" />
<meta name="DC.Title" content="Generalized Linear Model Functions" />
<meta name="DC.Format" content="XHTML" />
<meta name="DC.Identifier" content="wss1507134739235" />
<meta name="DC.Language" content="en-us" />

<link rel="stylesheet" type="text/css" href="css/normal_styles.css" />
<title>Generalized Linear Model Functions</title>
</head>
<body id="wss1507134739235">

	<h1 class="title topictitle1" id="ariaid-title1">Generalized Linear Model Functions</h1>

	<div class="body conbody"><div class="section" id="wss1507134739235__section_ntj_l24_xcb"><p class="p">The GLM and GLML1L2 functions perform linear regression analysis for distribution functions using a user-specified distribution family and link function. Their output is input to the GLMPredict_MLE and GLML1L2Predict functions (respectively), which perform generalized linear model prediction on new input data.</p>
<p class="p">The GLM and GLML1L2 functions differ in these ways:</p>

<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="wss1507134739235__table_vcg_pyn_xcb" class="table" frame="border" border="1" rules="all"><div class="caption"></div><colgroup><col style="width:25%" /><col style="width:25%" /><col style="width:25%" /><col style="width:25%" /></colgroup><thead class="thead" style="text-align:left;"><tr class="row"><th class="entry cellrowborder" style="vertical-align:top;" id="d206963e27">Function</th>
<th class="entry cellrowborder" style="vertical-align:top;" id="d206963e29">Supported Distribution Families</th>
<th class="entry cellrowborder" style="vertical-align:top;" id="d206963e31">Supported Regularization Models</th>
<th class="entry cellrowborder" style="vertical-align:top;" id="d206963e33">Output Tables</th>
</tr>
</thead>
<tbody class="tbody"><tr class="row"><td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e27 "><a class="xref" href="eej1558472403086.html#hrv1507149150084" title="This Machine Learning Engine function extends the linear regression model to enable linear equation to relate to dependent variables by a link function.">GLM</a></td>
<td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e29 ">See <a class="xref" href="eej1558472403086.html#miq1507149222649" title="">Supported Family/Link Function Combinations</a></td>
<td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e31 ">None</td>
<td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e33 "><ul class="ul" id="wss1507134739235__ul_p3z_h14_xcb"><li class="li">Model table</li>
</ul>
</td>
</tr>
<tr class="row"><td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e27 "><a class="xref" href="mxy1558472465230.html#fgw1518542561108" title="This Machine Learning Engine function differs from GLM in supporting ridge, LASSO, and elastic net regularization models and outputting optional factor table.">GLML1L2</a></td>
<td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e29 ">Binomial, Gaussian</td>
<td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e31 "><p class="p">Ridge, LASSO, and elastic net</p>
</td>
<td class="entry cellrowborder" style="vertical-align:top;" headers="d206963e33 "><ul class="ul" id="wss1507134739235__ul_emk_l24_xcb"><li class="li">Model table</li>
<li class="li">[Optional] Factor table</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="wss1507134739235__section_bh5_4sm_sbb"><h2 class="title sectiontitle">Regularization</h2><p class="p"><dfn class="term">Regularization</dfn> is a technique for reducing overfitting and thus decreasing the variance of trained models. Generalized linear models (GLM) are fit by minimizing a loss function, such as the sum of squared errors. For example, given a predictor vector <var class="keyword varname">X</var> ϵ <img class="image" id="wss1507134739235__image_bpr_xth_4z" src="gnf1492104149755.png" /><span class="ph sup"><var class="keyword varname">p</var></span>, a response variable <var class="keyword varname">Y</var> ϵ <img class="image" id="wss1507134739235__image_khk_15h_4z" src="gnf1492104149755.png" />, and <var class="keyword varname">N</var> observation pairs, you can find model parameters <var class="keyword varname">β</var><span class="ph sub">0</span> and <span class="ph b"><var class="keyword varname">β</var></span> with this formula:</p>
<div class="fig fignone" id="wss1507134739235__fig_mcd_gtl_nz"><div class="caption"></div><br /><img class="image" id="wss1507134739235__image_wrv_vtl_nz" src="hcd1491842531658.png" alt="Formula for finding model parameters β0 and β. Used by Machine Learning Engine generalized linear model functions." /><br /></div>
<p class="p">These fits can be regularized by adding a penalty function <var class="keyword varname">P</var>(<var class="keyword varname">β</var> ) to the loss function being minimized. For example:</p>
<div class="fig fignone" id="wss1507134739235__fig_bgz_rrl_nz"><div class="caption"></div><br /><img class="image" id="wss1507134739235__image_npg_ytl_nz" src="vye1491842707665.png" alt="Formula that adds penalty to loss function being minimized. Used by Machine Learning Engine generalized linear model functions." /><br /></div>
<p class="p">where λ controls the strength of the penalty function.</p>
<p class="p">For logistic regression, the loss function is based on the log likelihood, as follows:</p>
<div class="fig fignone" id="wss1507134739235__fig_pbj_pjr_mz"><div class="caption"></div><br /><img class="image" id="wss1507134739235__image_td1_qjr_mz" src="fua1491599905791.png" alt="Formula for loss function based on the log likelihood. Used by Machine Learning Engine generalized linear model functions." /><br /></div>
<div class="p">These are three popular penalty functions:<ul class="ul" id="wss1507134739235__ul_bz1_xl3_mz"><li class="li">The sum of the absolute values of the model parameters:<div class="fig fignone" id="wss1507134739235__fig_ezz_srl_nz"><div class="caption"></div><br /><img class="image" id="wss1507134739235__image_e5n_15l_nz" src="dfr1491842751626.png" alt="Formula for penalty function. Equals sum of absolute values of model parameters. Used by Machine Learning Engine generalized linear model functions." /><br /></div>
<p class="p">which is the L1 norm of the model parameters. This regularization technique, also called Least Absolute Shrinkage and Selection Operator (LASSO), was introduced by Robert Tibshirani in 1996. LASSO has the potential to shrink some parameters to zero; therefore, you can also use it for variable selection.</p>
</li>
<li class="li">The sum of the squared values of the model parameters:<div class="fig fignone" id="wss1507134739235__fig_yw2_trl_nz"><div class="caption"></div><br /><img class="image" id="wss1507134739235__image_akg_c5l_nz" src="iej1491842781828.png" alt="Formula for penalty function. Equals squared values of model parameters. Used by Machine Learning Engine generalized linear model functions." /><br /></div>
<p class="p">which is the L2 norm of the model parameters. This regularization technique is also called <dfn class="term">ridge regression</dfn>. With ridge regression, parameter values become smaller as λ increases, but never reach zero.</p>
</li>
<li class="li">Elastic net regularization, which is a linear combination of L1 and L2 normalization:<div class="fig fignone" id="wss1507134739235__fig_j4l_trl_nz"><div class="caption"></div><br /><img class="image" id="wss1507134739235__image_g3n_d5l_nz" src="sox1491842814356.png" alt="Formula for penalty function elastic net regularization. Used by Machine Learning Engine generalized linear model functions." /><br /></div>
</li>
</ul>
</div>
</div>
<div class="section" id="wss1507134739235__section_s2s_lk3_mz"><h2 class="title sectiontitle">References</h2><ul class="ul" id="wss1507134739235__ul_zjj_km3_mz"><li class="li"><p class="p"><cite class="cite">Friedman, J., Hastie, T., and Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1 - 22.doi</cite> (<a class="xref" href="http://dx.doi.org/10.18637/jss.v033.i01" target="_blank" title="">GLM regularization paths article</a>)</p>
</li>
<li class="li"><p class="p"><cite class="cite">Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J. and Tibshirani, R. J. (2012), Strong rules for discarding predictors in lasso-type problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74: 245–266. doi:10.1111/j.1467-9868.2011.01004.x</cite></p>
</li>
</ul>
</div>
</div>

</body>
</html>